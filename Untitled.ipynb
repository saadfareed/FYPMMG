{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['saad is my name', ' I am 18 years of age', ' My father name is Faiz fareed', '']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'saad': 1, 'is': 2, 'my': 1, 'name': 2, 'I': 1, 'am': 1, '18': 1, 'years': 1, 'of': 1, 'age': 1, 'My': 1, 'father': 1, 'Faiz': 1, 'fareed': 1}\n",
      "['saad', 'is', 'my', 'name', 'I', 'am', '18', 'years', 'of', 'age', 'My', 'father', 'Faiz', 'fareed']\n"
     ]
    }
   ],
   "source": [
    "sentence=\"saad is my name. I am 18 years of age. My father name is Faiz fareed.\"\n",
    "processed_text=sentence.split(\".\")\n",
    "DF = {}\n",
    "for i in range(len(processed_text)):\n",
    "    tokens = processed_text[i]\n",
    "    for w in tokens.split():\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "            \n",
    "for i in DF:\n",
    "    DF[i]=len(DF[i])\n",
    "print(DF)\n",
    "total_voc=[x for x in DF]\n",
    "print(total_voc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'saad': {0}, 'is': {0, 2}, 'my': {0}, 'name': {0, 2}, 'I': {1}, 'am': {1}, '18': {1}, 'years': {1}, 'of': {1}, 'age': {1}, 'My': {2}, 'father': {2}, 'Faiz': {2}, 'fareed': {2}}\n"
     ]
    }
   ],
   "source": [
    "sentence=\"saad is my is name. I am 18 years of age. My father name is Faiz fareed.\"\n",
    "processed_text=sentence.split(\".\")\n",
    "DF = {}\n",
    "for i in range(len(processed_text)):\n",
    "    tokens = processed_text[i]\n",
    "    tokens=tokens.split(\"''\")\n",
    "    for j in range(len(tokens)):\n",
    "        tokens=tokens[j]\n",
    "        for w in tokens.split():\n",
    "            try:\n",
    "                DF[w].add(i)\n",
    "            except:\n",
    "                DF[w] = {i}\n",
    "print(DF)\n",
    "#total_voc=[x for x in DF]\n",
    "#print(total_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'saad is my is name': {0, 3}, ' I am 18 years of age': {1}, ' My father name is Faiz fareed': {2}}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "sentence=\"saad is my is name. I am 18 years of age. My father name is Faiz fareed.saad is my is name\"\n",
    "processed_text=sentence.split(\".\")\n",
    "DF = {}\n",
    "for i in range(len(processed_text)):\n",
    "    tokens = processed_text[i]\n",
    "    tokens=tokens.split(\"''\")\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "print(DF)\n",
    "score_len=len(processed_text)\n",
    "print(score_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN|The NN|live-audio NN|app NN|allows NN|users NN|to NN|start NN|a NN|virtual NN|room NN|or NN|join NN|one NN|in NN|which NN|other NN|people NN|are NN|speaking. NN|In NN|these NN|recordings, NN|men NN|were NN|auctioning NN|off NN|parts NN|of NN|women PU|' NN|s NN|bodies, NN|including NN|hers. NN|But NN|Bhavini, NN|who NN|lives NN|in NN|India NN|and NN|whose NN|name NN|has NN|been NN|changed NN|to NN|protect NN|her NN|identity, NN|was NN|not NN|surprised. PU|\" NN|Men NN|have NN|been NN|harassing NN|me NN|on NN|the NN|app NN|for NN|months, NN|making NN|rooms NN|using NN|my NN|name NN|at NN|least NN|once NN|a NN|week NN|and NN|calling NN|me NN|worse NN|things. NN|This NN|is NN|the NN|price NN|you NN|pay NN|for NN|being NN|a NN|vocal NN|woman, PU|\" NN|the NN|33-year-old NN|policy NN|researcher NN|said. NN|The NN|reach NN|of NN|the NN|winter NN|games NN|in NN|China NN|is NN|extending NN|far NN|beyond NN|Beijing. NN|When NN|I NN|get NN|through NN|to NN|online NN|influencer NN|Yao NN|via NN|video NN|call NN|he NN|is NN|standing NN|on NN|top NN|of NN|a NN|mountain NN|at NN|sunset, NN|about NN|to NN|snowboard NN|down. NN|It NN|is NN|an NN|image NN|of NN|China NN|that NN|you PU|' NN|re NN|going NN|to NN|see NN|a NN|lot NN|of NN|over NN|the NN|next NN|few NN|weeks NN|as NN|the NN|Winter NN|Olympics NN|kicks NN|off; NN|beautiful, NN|snow-covered NN|slopes. NN|But NN|Yao NN|isn PU|' NN|t NN|anywhere NN|near NN|the NN|host NN|city NN|of NN|Beijing. NN|He NN|is NN|in NN|Xinjiang PU|- NN|the NN|region NN|with, NN|arguably, NN|the NN|best NN|snow NN|and NN|the NN|best NN|climate NN|in NN|China. NN|It NN|is NN|also NN|the NN|region NN|where NN|the NN|US NN|and NN|others NN|say NN|China NN|is NN|committing NN|genocide NN|against NN|its NN|minority NN|Uyghur NN|population. NN|As NN|the NN|Olympics NN|approach, NN|a NN|government NN|push NN|to NN|promote NN|Xinjiang NN|as NN|a NN|snow NN|sport NN|destination NN|has NN|been NN|stepped NN|up. NN|Images NN|of NN|horse-drawn NN|sleds NN|passing NN|snow-covered NN|wooden NN|huts, NN|combined NN|with NN|skiers NN|at NN|test NN|events, NN|have NN|been NN|heavily NN|featured NN|in NN|state NN|media. NN|It PU|' NN|s NN|almost NN|as NN|if NN|this NN|troubled NN|region NN|is NN|part NN|of NN|the NN|Games. \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "accessToken = \"bf8426d5-7e47-4f3d-9802-fc827f74cfb5\"\n",
    "file=open(\"BBC.txt\",'r')\n",
    "frequency={}\n",
    "sadi={}\n",
    "inputText=file.read()\n",
    "JSON_MSG = \"{ \\\"text\\\" : \\\"\" + inputText + \"\\\" , \\\"token\\\" : \\\"\" + accessToken + \"\\\" }\"\n",
    "#print(JSON_MSG)\n",
    "response = requests.post('https://api.cle.org.pk/v1/pos',json={\"text\":inputText,\"token\":accessToken})\n",
    "response=response.json()\n",
    "with open('pos.json','w') as f:\n",
    "    json.dump(response,f)\n",
    "f.close()\n",
    "with open('pos.json',\"r\") as w:\n",
    "    saad=json.load(w)\n",
    "w.close()\n",
    "#print(type(saad))\n",
    "fareed=saad['response']\n",
    "sadi=(fareed['tagged_text'])\n",
    "print(sadi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It is an image of China that you're going to see a lot of over the next few weeks as the Winter Olympics kicks off; beautiful, snow-covered slopes\n",
      "\n",
      "\n",
      "When I get through to online influencer Yao via video call he is standing on top of a mountain at sunset, about to snowboard down\n",
      "\n",
      "\n",
      "\"Men have been harassing me on the app for months, making rooms using my name at least once a week and calling me worse things\n",
      "\n",
      "\n",
      "Images of horse-drawn sleds passing snow-covered wooden huts, combined with skiers at test events, have been heavily featured in state media\n",
      " It is also the region where the US and others say China is committing genocide against its minority Uyghur population\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "sentence_li=[]\n",
    "x = sadi.split(\".\")\n",
    "DF={}\n",
    "#print(x)\n",
    "for i in range(len(x)):\n",
    "    #print(x[i])\n",
    "    sentence_li.append(x[i])\n",
    "   # print(\"\\n\")\n",
    "#print(sentence_li)\n",
    "#print(len(sentence_li))\n",
    "\n",
    "NN_li=[]\n",
    "NNP_li=[]\n",
    "other_li=[]\n",
    "for i in range(len(sentence_li)):\n",
    "    sentence=str(sentence_li[i])\n",
    "    s=sentence.split(\" \")\n",
    "    NN=0\n",
    "    NNP=0\n",
    "    other=0\n",
    "    for w in s:\n",
    "        f=str(w)\n",
    "        f=f.split(\"|\")\n",
    "        #print(f)\n",
    "        for w in f[::2]:\n",
    "            #print(w)\n",
    "            if(w==\"NN\"):\n",
    "                NN=NN+1\n",
    "            elif(w==\"NNP\"):\n",
    "                NNP=NNP+1\n",
    "            else:\n",
    "                other=other+1\n",
    "      \n",
    "    NN_li.append(NN)\n",
    "    NNP_li.append(NNP)\n",
    "    other_li.append(other)\n",
    "                 \n",
    "\n",
    "#print(other_li)\n",
    "#sort_index = numpy.argsort(other_li)[::-1]\n",
    "#print(sort_index)\n",
    "#print(NN_li)\n",
    "fi=open(\"BBC.txt\",'r')\n",
    "textfile=fi.read()\n",
    "tex=textfile.split(\".\")\n",
    "sort_index = numpy.argsort(NN_li)[::-1]\n",
    "#print(sort_index)\n",
    "#print(sort_index)\n",
    "sort_index=list(sort_index)\n",
    "length = len(sort_index)\n",
    "sum_index = length//3\n",
    "summary = sort_index[:sum_index]\n",
    "#print(summary)\n",
    "for i in summary:\n",
    "    print(tex[i])\n",
    "\n",
    "#print(result[:len(result)])\n",
    "#print(r)\n",
    "\n",
    "#print(NNP_li)\n",
    "#sort_index = numpy.argsort(NNP_li)[::-1]\n",
    "#print(sort_index)\n",
    "#print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "list1 = [1,2,3]\n",
    "list2 = [1,2,3]\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    list1[i]=list1[i] + list2[i]\n",
    "\n",
    "print(list1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 14, 19, 25, 16, 13, 25, 29, 11, 17, 20, 19, 21, 13, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[19, 14, 19, 25, 16, 13, 25, 29, 11, 17, 20, 19, 21, 13, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "sentence_li=[]\n",
    "x = sadi.split(\".\")\n",
    "DF={}\n",
    "#print(x)\n",
    "for i in range(len(x)):\n",
    "    #print(x[i])\n",
    "    sentence_li.append(x[i])\n",
    "   # print(\"\\n\")\n",
    "#print(sentence_li)\n",
    "#print(len(sentence_li))\n",
    "\n",
    "NN_li=[]\n",
    "NNP_li=[]\n",
    "other_li=[]\n",
    "for i in range(len(sentence_li)):\n",
    "    sentence=str(sentence_li[i])\n",
    "    s=sentence.split(\" \")\n",
    "    NN=0\n",
    "    NNP=0\n",
    "    other=0\n",
    "    for w in s:\n",
    "        f=str(w)\n",
    "        f=f.split(\"|\")\n",
    "        #print(f)\n",
    "        for w in f[::2]:\n",
    "            #print(w)\n",
    "            if(w==\"NN\"):\n",
    "                NN=NN+1\n",
    "            elif(w==\"NNP\"):\n",
    "                NNP=NNP+1\n",
    "            else:\n",
    "                other=other+1\n",
    "      \n",
    "    NN_li.append(NN)\n",
    "    NNP_li.append(NNP)\n",
    "    other_li.append(other)\n",
    "\n",
    "print(NN_li)\n",
    "print(NNP_li)\n",
    "result=NN_li\n",
    "for i in range(len(result)):\n",
    "    result[i]=result[i]+NNP_li[i]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It is an image of China that you're going to see a lot of over the next few weeks as the Winter Olympics kicks off; beautiful, snow-covered slopes\n",
      "\n",
      "\n",
      "When I get through to online influencer Yao via video call he is standing on top of a mountain at sunset, about to snowboard down\n",
      "\n",
      "\n",
      "\"Men have been harassing me on the app for months, making rooms using my name at least once a week and calling me worse things\n",
      "\n",
      "\n",
      "Images of horse-drawn sleds passing snow-covered wooden huts, combined with skiers at test events, have been heavily featured in state media\n",
      " It is also the region where the US and others say China is committing genocide against its minority Uyghur population\n"
     ]
    }
   ],
   "source": [
    "fi=open(\"BBC.txt\",'r')\n",
    "textfile=fi.read()\n",
    "tex=textfile.split(\".\")\n",
    "sort_index = numpy.argsort(result)[::-1]\n",
    "#print(sort_index)\n",
    "sort_index=list(sort_index)\n",
    "length = len(sort_index)\n",
    "sum_index = length//3\n",
    "summary = sort_index[:sum_index]\n",
    "#print(summary)\n",
    "for i in summary:\n",
    "    print(tex[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary...\n",
      " It is an image of China that you're going to see a lot of over the next few weeks as the Winter Olympics kicks off; beautiful, snow-covered slopes\n",
      "Images of horse-drawn sleds passing snow-covered wooden huts, combined with skiers at test events, have been heavily featured in state media\n",
      "\n",
      "The reach of the winter games in China is extending far beyond Beijing\n",
      "\n",
      "In these recordings, men were auctioning off parts of women's bodies, including hers\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "final = []\n",
    "file=open(\"BBC.txt\",'r')\n",
    "inputText=file.read()\n",
    "sentence=inputText\n",
    "sentlen=len(sentence)\n",
    "saad=sentence.split(\".\")\n",
    "#print(len(saad))\n",
    "#print(\"\\n\")\n",
    "result=[]\n",
    "def sentencelen(saad):\n",
    "    for i in range(len(saad)):\n",
    "        ilen=len(saad[i])\n",
    "        #print(ilen)\n",
    "        alpha=ilen**0.5\n",
    "        #print(alpha)\n",
    "        if(alpha==0):\n",
    "            continue\n",
    "        beta=1/alpha\n",
    "        results=final.append(beta)\n",
    "        #print(\"\\n\")\n",
    "    maximum=max(final)\n",
    "    #print(maximum)\n",
    "    for i in final:\n",
    "        gema=i/maximum\n",
    "        #print(gema)\n",
    "        res=result.append(gema)\n",
    "    return 0\n",
    "sentencelen(saad)\n",
    "\n",
    "        \n",
    "sort_index = numpy.argsort(result)[::-1]\n",
    "#print(sort_index)\n",
    "sort_index=list(sort_index)\n",
    "length = len(sort_index)\n",
    "#print(length)\n",
    "sum_index = length//3\n",
    "#print(sum_index)\n",
    "summary = sort_index[:sum_index]\n",
    "#print(summary)\n",
    "print(\"Summary...\")\n",
    "for i in summary:\n",
    "    print(saad[i])\n",
    "\n",
    "#print(\"\\n\")\n",
    "#print(saad[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 14, 19, 25, 16, 13, 0, 25, 29, 10, 17, 20, 18, 20, 13, 0]\n",
      "[8, 7, 3, 13, 11]\n",
      "Summary...\n",
      "\n",
      "Images of horse-drawn sleds passing snow-covered wooden huts, combined with skiers at test events, have been heavily featured in state media\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "accessToken = \"bf8426d5-7e47-4f3d-9802-fc827f74cfb5\"\n",
    "file=open(\"BBC.txt\",'r')\n",
    "frequency={}\n",
    "sadi={}\n",
    "inputText=file.read()\n",
    "JSON_MSG = \"{ \\\"text\\\" : \\\"\" + inputText + \"\\\" , \\\"token\\\" : \\\"\" + accessToken + \"\\\" }\"\n",
    "#print(JSON_MSG)\n",
    "response = requests.post('https://api.cle.org.pk/v1/pos',json={\"text\":inputText,\"token\":accessToken})\n",
    "response=response.json()\n",
    "with open('pos.json','w') as f:\n",
    "    json.dump(response,f)\n",
    "f.close()\n",
    "with open('pos.json',\"r\") as w:\n",
    "    saad=json.load(w)\n",
    "w.close()\n",
    "#print(type(saad))\n",
    "fareed=saad['response']\n",
    "sadi=(fareed['tagged_text'])\n",
    "#print(sadi)\n",
    "import numpy\n",
    "sentence_li=[]\n",
    "x = sadi.split(\".\")\n",
    "DF={}\n",
    "#print(x)\n",
    "for i in range(len(x)):\n",
    "    #print(x[i])\n",
    "    sentence_li.append(x[i])\n",
    "   # print(\"\\n\")\n",
    "#print(sentence_li)\n",
    "#print(len(sentence_li))\n",
    "\n",
    "NN_li=[]\n",
    "NNP_li=[]\n",
    "other_li=[]\n",
    "for i in range(len(sentence_li)):\n",
    "    sentence=str(sentence_li[i])\n",
    "    s=sentence.split(\" \")\n",
    "    NN=0\n",
    "    NNP=0\n",
    "    other=0\n",
    "    for w in s:\n",
    "        f=str(w)\n",
    "        f=f.split(\"|\")\n",
    "        #print(f)\n",
    "        for w in f[::2]:\n",
    "            #print(w)\n",
    "            if(w==\"NN\"):\n",
    "                NN=NN+1\n",
    "            elif(w==\"NNP\"):\n",
    "                NNP=NNP+1\n",
    "            else:\n",
    "                other=other+1\n",
    "      \n",
    "    NN_li.append(NN)\n",
    "    NNP_li.append(NNP)\n",
    "    other_li.append(other)\n",
    "    \n",
    "#print(\"List of Noun Singular in sentence: \")\n",
    "#print(NN_li)\n",
    "#print(\"\\n\")\n",
    "#print(\"List of Proper Noun Singular in sentence: \")\n",
    "#print(NNP_li)\n",
    "#print(\"\\n\")\n",
    "result=NN_li\n",
    "for i in range(len(result)):\n",
    "    result[i]=result[i]+NNP_li[i]\n",
    "    \n",
    "#print(\"Resultant Score list of sentence: \")\n",
    "print(result)\n",
    "#print(\"\\n\")\n",
    "fi=open(\"BBC.txt\",'r')\n",
    "textfile=fi.read()\n",
    "tex=textfile.split(\".\")\n",
    "sort_index = numpy.argsort(result)[::-1]\n",
    "#print(sort_index)\n",
    "sort_index=list(sort_index)\n",
    "length = len(sort_index)\n",
    "sum_index = length//3\n",
    "summary = sort_index[:sum_index]\n",
    "print(summary)\n",
    "#print(\"\\n\")\n",
    "print(\"Summary...\\n\")\n",
    "#for i in summary:\n",
    "#    print(tex[i])\n",
    "#print(tex[8])\n",
    "#print(tex[7])\n",
    "#print(tex[3])\n",
    "print(tex[13])\n",
    "#print(tex[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
